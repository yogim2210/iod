{"cells":[{"cell_type":"markdown","metadata":{"id":"Nbs82Dhcoygp"},"source":["<div>\n","<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"mDkjMtYuoygt"},"source":["# Demo 7.2.4: Ensemble Methods\n","\n","INSTRUCTIONS:\n","\n","- Run the cells\n","- Observe and understand the results\n","- Answer the questions"]},{"cell_type":"markdown","metadata":{"id":"X_BockGZoygv"},"source":["This is an excerpt from the [Introduction to Ensembling/Stacking in Python\n","](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) by **Anisotropic** on **Kaggle**.\n","Credits also to:\n","- **Faron**: Mathias Müller, Data Scientist @ H2O.ai, Diplom (eq. MSc.) in Computer Science | HU-Berlin (zhryyrezng84@tznvy.pbz)\n","- **Sina**: AI Student at IUSTTehran, Tehran Province, Iran (https://sinakhorami.github.io)"]},{"cell_type":"markdown","metadata":{"id":"yHcwNgdRoygz"},"source":["## Dataset: Titanic\n","The full Titanic dataset is available from the Department of Biostatistics at the [Vanderbilt University School of Medicine](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv) in several formats. \n","\n","The [Encyclopedia Titanica](https://www.encyclopedia-titanica.org/) is the website of reference regarding the Titanic. It contains all the facts, history, and data surrounding the Titanic, including a full list of passengers and crew members. \n","\n","The Titanic dataset is also the subject of the introductory competition on [Kaggle.com](https://www.kaggle.com/c/titanic). Requires opening an account.\n","\n","### Overview\n","The data has been split into two groups:\n","- Training set (`train.csv`)\n","- Test set (`test.csv`)\n","\n","The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n","\n","The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n","\n","### Data Dictionary\n","- **PassengerID**: A column added by Kaggle to identify each row and make submissions easier\n","- **Survived**: Whether the passenger survived or not and the value we are predicting (`0`=No, `1`=Yes)\n","- **Pclass**: The class of the ticket the passenger purchased (`1`=1st, `2`=2nd, `3`=3rd)\n","- **Sex**: The passenger's sex\n","- **Age**: The passenger's age in years\n","- **SibSp**: The number of siblings or spouses the passenger had aboard the Titanic\n","- **Parch**: The number of parents or children the passenger had aboard the Titanic\n","- **Ticket**: The passenger's ticket number\n","- **Fare**: The fare the passenger paid\n","- **Cabin**: The passenger's cabin number\n","- **Embarked**: The port where the passenger embarked `C`=Cherbourg, `Q`=Queenstown, `S`=Southampton)\n","\n","### Variable Notes\n","- **pclass**: A proxy for socio-economic status (SES)\n","    - _1st_ = Upper\n","    - _2nd_ = Middle\n","    - _3rd_ = Lower\n","- **age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n","- **sibsp**: The dataset defines family relations in this way\n","    - _Sibling_ = brother, sister, stepbrother, stepsister\n","    - _Spouse_ = husband, wife (mistresses and fiancés were ignored)\n","- **parch**: The dataset defines family relations in this way\n","    - _Parent_ = mother, father\n","    - _Child_ = daughter, son, stepdaughter, stepson\n","    - Some children travelled only with a nanny, therefore `parch=0` for them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PWeqqQhoyg2"},"outputs":[],"source":["## Import Libraries\n","\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.model_selection import KFold\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import auc\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","\n","import xgboost as xgb\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"v2bpkZU2oyhA"},"source":["## Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxEPE-yLoyhB"},"outputs":[],"source":["## Loading the dataset\n","\n","# Load in the train and test datasets\n","train = pd.read_csv('../../data/titanic_train.csv')\n","test = pd.read_csv('../../data/titanic_test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIoGVUAboyhE"},"outputs":[],"source":["## Check the data\n","\n","# About train\n","print(train.info())\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mh4D5HywoyhI"},"outputs":[],"source":["# About test\n","print(test.info())\n","test.head()"]},{"cell_type":"markdown","metadata":{"id":"v2qEcKU4oyhM"},"source":["## Data Cleaning, Exploration and Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhTkMdidoyhN"},"outputs":[],"source":["# Extract titles from passenger names\n","def get_title(name):\n","    title_search = re.search(' ([A-Za-z]+)\\.', name)\n","    # If the title exists, extract and return it.\n","    if title_search:\n","        return title_search.group(1)\n","    return ''\n","\n","full_data = [train, test]\n","# calculate quantiles\n","fare_quant = train.Fare.quantile([.25, .5, .75])\n","age_quant = train.Age.quantile([0.2, 0.4, 0.6, 0.8])\n","\n","verbose = True\n","\n","# Gives the length of the name\n","for dataset in full_data:\n","    dataset['Name_length'] = dataset['Name'].apply(len)\n","\n","# Feature that tells whether a passenger had a cabin on the Titanic\n","for dataset in full_data:\n","    dataset['Has_Cabin'] = dataset['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n","\n","# Create new feature FamilySize as a combination of SibSp and Parch\n","for dataset in full_data:\n","    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n","\n","# Create new feature IsAlone from FamilySize\n","for dataset in full_data:\n","    dataset['IsAlone'] = 0\n","    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n","\n","# Remove all NULLS in the Embarked column\n","for dataset in full_data:\n","    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n","\n","# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n","for dataset in full_data:\n","    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n","# train['CategoricalFare'] = pd.qcut(train['Fare'], 4, duplicates = 'drop')\n","\n","# Create a New feature CategoricalAge\n","for dataset in full_data:\n","    age_avg = dataset['Age'].mean()\n","    age_std = dataset['Age'].std()\n","    dataset['Age'] = dataset['Age'].apply(lambda x: x if x is None else (np.random.randint(age_avg - age_std, age_avg + age_std)))\n","    dataset['Age'] = dataset['Age'].astype(int)\n","# train['CategoricalAge'] = pd.cut(train['Age'], 5, duplicates = 'drop')\n","\n","# Create a new feature Title, containing the titles of passenger names\n","for dataset in full_data:\n","    dataset['Title'] = dataset['Name'].apply(get_title)\n","\n","# Group all non-common titles into one single grouping 'Rare'\n","for dataset in full_data:\n","    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n","    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n","    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n","    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","\n","for dataset in full_data:\n","    # Mapping Sex\n","    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n","    \n","    # Mapping titles\n","    title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n","    dataset['Title'] = dataset['Title'].map(title_mapping)\n","    dataset['Title'] = dataset['Title'].fillna(0)\n","    \n","    # Mapping Embarked\n","    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n","    \n","    # Mapping Fare\n","#     dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']                               = 0\n","#     dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n","#     dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n","#     dataset.loc[ dataset['Fare'] > 31, 'Fare']                                  = 3\n","#     dataset['Fare'] = dataset['Fare'].astype(int)\n","    for i in range(len(fare_quant) + 1):\n","        if i == 0:\n","            p = fare_quant.iloc[i]\n","            dataset.loc[ dataset['Fare'] <= p, 'Fare'] = i # 0\n","            if verbose:\n","                print('Categorising Fare')\n","                print('%d: > %7.4f and <= %7.4f ' % (i, dataset['Fare'].min(), p))\n","            continue\n","        if i == len(fare_quant):\n","            p_1 = fare_quant.iloc[i - 1]\n","            dataset.loc[ dataset['Fare'] > p_1, 'Fare'] = i # len(quant)\n","            if verbose:\n","                print('%d: > %7.4f' % (i, p_1))\n","            continue\n","        if i < len(fare_quant):\n","            p = fare_quant.iloc[i]\n","            p_1 = fare_quant.iloc[i - 1]\n","            dataset.loc[(dataset['Fare'] > p_1) & (dataset['Fare'] <= p), 'Fare'] = i\n","            if verbose:\n","                print('%d: > %7.4f and <= %7.4f' % (i, p_1, p))\n","    \n","    # Mapping Age\n","#     dataset.loc[ dataset['Age'] <= 16, 'Age']                          = 0\n","#     dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n","#     dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n","#     dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n","#     dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n","    for i in range(len(age_quant) + 1):\n","        if i == 0:\n","            p = age_quant.iloc[i]\n","            dataset.loc[ dataset['Age'] <= p, 'Age'] = i # 0\n","            if verbose:\n","                print('Categorising Age')\n","                print('%d: > %7.4f and <= %7.4f ' % (i, dataset['Age'].min(), p))\n","            continue\n","        if i == len(age_quant):\n","            p_1 = age_quant.iloc[i - 1]\n","            dataset.loc[ dataset['Age'] > p_1, 'Age'] = i # len(quant)\n","            if verbose:\n","                print('%d: > %7.4f' % (i, p_1))\n","            continue\n","        if i < len(age_quant):\n","            p = age_quant.iloc[i]\n","            p_1 = age_quant.iloc[i - 1]\n","            dataset.loc[(dataset['Age'] > p_1) & (dataset['Age'] <= p), 'Age'] = i\n","            if verbose:\n","                print('%d: > %7.4f and <= %7.4f' % (i, p_1, p))\n","\n","    verbose = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ysxWxkjoyhQ"},"outputs":[],"source":["# Feature selection\n","drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n","train = train.drop(drop_elements, axis = 1)\n","# train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n","test  = test.drop(drop_elements, axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"su-JQwAMoyhS"},"outputs":[],"source":["print(train.info())\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HB7S6e7oyhV"},"outputs":[],"source":["print(test.info())\n","test.head()"]},{"cell_type":"markdown","metadata":{"id":"auRarlj-oyhX"},"source":["## Pearson Correlation Heatmap\n","Check correlation of the features to see how related one feature is to the next."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q83j7lJKoyhX"},"outputs":[],"source":["colormap = plt.cm.RdBu\n","plt.figure(figsize = (14, 12))\n","plt.title('Pearson Correlation of Features', size = 15)\n","sns.heatmap(train.astype(float).corr(),\n","            linewidths = 0.1,\n","            vmax = 1.0,\n","            square = True,\n","            cmap = colormap,\n","            linecolor = 'white',\n","            annot = True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cu74cJYmoyhZ"},"source":["**QUESTION**: What does the chart say?\n","\n","The Pearson Correlation plot can tell that there are not too many features strongly correlated with one another. \n","\n","This is good because:\n","- this means that there is not much redundant or superfluous data in the training set\n","- each feature carries with it some unique information\n","\n","The two most correlated features are `FamilySize` and `Parch` (Parents and Children)"]},{"cell_type":"markdown","metadata":{"id":"jsv3kLqDoyha"},"source":["## Pairplots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SH1-kc2poyha"},"outputs":[],"source":["features = ['Survived', 'Pclass', 'Sex', 'Age', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'Title']\n","g = sns.pairplot(\n","    train[features],\n","    hue = 'Survived',\n","    palette = 'seismic',\n","    size = 1.2,\n","    diag_kind = 'kde',\n","    diag_kws = dict(shade = True),\n","    plot_kws = dict(s = 10))\n","g.set(xticklabels = [])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NhZU-qh0oyhd"},"source":["## Ensembling & Stacking models\n","\n","#### Helpers via Python Classes\n","Here we invoke the use of Python's classes to help make it more convenient. In short, a class helps to extend some code/program for creating objects as well as to implement functions and methods specific to that class.\n","\n","In the code below, a class `SklearnHelper` allows to extend the inbuilt methods (such as train, predict and fit) common to all the `Scikit-Learn` classifiers. This cuts out redundancy as it avoids to write the same methods many times when invoking many different classifiers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CljXJgqRoyhe"},"outputs":[],"source":["# Some common parameters\n","ntrain = train.shape[0]\n","ntest = test.shape[0]\n","SEED = 0 # for reproducibility\n","NFOLDS = 5 # set folds for out-of-fold prediction\n","kf = KFold(n_splits = NFOLDS)\n","\n","# Class to extend the Sklearn classifier\n","class SklearnHelper(object):\n","    def __init__(self, clf, seed = 0, params = None):\n","        params['random_state'] = seed\n","        self.clf = clf(**params)\n","\n","    def train(self, x_train, y_train):\n","        self.clf.fit(x_train, y_train)\n","\n","    def predict(self, x):\n","        return self.clf.predict(x)\n","    \n","    def predict_proba(self, x):\n","        return self.clf.predict_proba(x)\n","    \n","    def fit(self, x, y):\n","        return self.clf.fit(x, y)\n","    \n","    def feature_importances(self, x, y):\n","        return self.clf.fit(x, y).feature_importances_"]},{"cell_type":"markdown","metadata":{"id":"Ayjsoj3foyhg"},"source":["### Out-of-Fold Predictions\n","Stacking uses predictions of base classifiers as input for training to a second level model.\n","\n","However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second level training.\n","\n","This runs the risk of the base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWakA1iboyhh"},"outputs":[],"source":["def get_oof(clf, x_train, y_train, x_test):\n","    oof_train = np.zeros((ntrain, ))\n","    oof_test = np.zeros((ntest, ))\n","    oof_test_skf = np.empty((NFOLDS, ntest))\n","\n","    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n","        x_tr = x_train[train_index]\n","        y_tr = y_train[train_index]\n","        x_te = x_train[test_index]\n","\n","        clf.train(x_tr, y_tr)\n","\n","        oof_train[test_index] = clf.predict(x_te)\n","        oof_test_skf[i, :] = clf.predict(x_test)\n","\n","    oof_test[:] = oof_test_skf.mean(axis = 0)\n","    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"]},{"cell_type":"markdown","metadata":{"id":"WBS-NaOtoyhj"},"source":["### Generating the Base First Level Models\n","The first level classification models can all be conveniently invoked via the `Scikit-Learn` library and are listed as follows:\n","\n","- Random Forest classifier\n","- Extra Trees classifier\n","- AdaBoost classifier\n","- Gradient Boosting classifier\n","- Support Vector classifier\n","\n","#### Parameters\n","\n","A quick summary of the parameters:\n","\n","- **n_jobs**: Number of cores used for the training process. If set to -1, all cores are used.\n","- **n_estimators**: Number of classification trees in your learning model (set to 10 per default).\n","- **max_depth**: Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep.\n","- **verbose**: Controls whether to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.\n","\n","Please check out the full description via the official [Scikit-Learn](http://scikit-learn.org/) website."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUJcTwfBoyhj"},"outputs":[],"source":["# Set the parameters for the classifiers\n","# Random Forest parameters\n","rf_params = {\n","    'n_jobs': -1,\n","    'n_estimators': 500,\n","    'warm_start': True,\n","#     'max_features': 0.2,\n","    'max_depth': 6,\n","    'min_samples_leaf': 2,\n","    'max_features': 'sqrt',\n","    'verbose': 0\n","}\n","\n","# Extra Trees parameters\n","et_params = {\n","    'n_jobs': -1,\n","    'n_estimators': 500,\n","#     'max_features': 0.5,\n","    'max_depth': 8,\n","    'min_samples_leaf': 2,\n","    'verbose': 0\n","}\n","\n","# AdaBoost parameters\n","ada_params = {\n","    'n_estimators': 500,\n","    'learning_rate' : 0.75\n","}\n","\n","# Gradient Boosting parameters\n","gb_params = {\n","    'n_estimators': 500,\n","#     'max_features': 0.2,\n","    'max_depth': 5,\n","    'min_samples_leaf': 2,\n","    'verbose': 0\n","}\n","\n","# Support Vector Classifier parameters \n","svc_params = {\n","    'kernel' : 'linear',\n","    'C' : 0.025,\n","    'probability' : True\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xk7qzaBBoyhk"},"outputs":[],"source":["# Create the objects that represent our the models\n","rf  = SklearnHelper(clf = RandomForestClassifier,     seed = SEED, params = rf_params)\n","et  = SklearnHelper(clf = ExtraTreesClassifier,       seed = SEED, params = et_params)\n","ada = SklearnHelper(clf = AdaBoostClassifier,         seed = SEED, params = ada_params)\n","gb  = SklearnHelper(clf = GradientBoostingClassifier, seed = SEED, params = gb_params)\n","svc = SklearnHelper(clf = SVC,                        seed = SEED, params = svc_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTCKIY5Koyhm"},"outputs":[],"source":["# Create Numpy arrays to feed into the models\n","y_train = train['Survived'].ravel()\n","train = train.drop(['Survived'], axis = 1)\n","x_train = train.values\n","x_test = test.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXSbHsQ6oyhn"},"outputs":[],"source":["# Create our OOF (Out-of-Fold) train and test predictions\n","# These base results will be used as new features\n","et_oof_train,  et_oof_test  = get_oof(et,  x_train, y_train, x_test) # Extra Trees\n","rf_oof_train,  rf_oof_test  = get_oof(rf,  x_train, y_train, x_test) # Random Forest\n","ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n","gb_oof_train,  gb_oof_test  = get_oof(gb,  x_train, y_train, x_test) # Gradient Boost\n","svc_oof_train, svc_oof_test = get_oof(svc, x_train, y_train, x_test) # Support Vector Classifier\n","\n","print('Training is complete')"]},{"cell_type":"markdown","metadata":{"id":"Sq0AQ5__oyho"},"source":["### Feature importance from the different classifiers\n","The importance of the various features in the training and test sets is available from the first level classifiers with one very simple line of code.\n","\n","As per the `Scikit-Learn` documentation, most of the classifiers are built in with an attribute which returns feature importances by refering to the `.featureimportances_` attribute.\n","\n","**NOTE**: SVC does not have the `.featureimportances_` attribute."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VARSwEThoyhp"},"outputs":[],"source":["# Create a dataframe with features\n","cols = train.columns.values\n","feature_dataframe = pd.DataFrame({\n","    'Feature':        cols,\n","    'Random Forest':  rf.feature_importances(x_train, y_train),\n","    'Extra Trees':    et.feature_importances(x_train, y_train),\n","    'AdaBoost':       ada.feature_importances(x_train, y_train),\n","    'Gradient Boost': gb.feature_importances(x_train, y_train)\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-N8IDM0oyhr"},"outputs":[],"source":["feature_dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wgblQ9yoyht"},"outputs":[],"source":["fig = plt.figure(figsize = (15, 6))\n","\n","ind = np.arange(len(cols)) \n","width = 0.2\n","\n","for i, m in enumerate(feature_dataframe.columns[1:]):\n","    plt.bar(ind + ((i - 1 - width) * width), feature_dataframe[m], width, label = m)\n","    \n","plt.title('Feature Importance')\n","plt.xticks(ind, cols, rotation = 45)\n","plt.ylabel('Score')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0bYuJ4ioyhv"},"outputs":[],"source":["# Create a new column containing the average of values\n","feature_dataframe['Mean'] = feature_dataframe.mean(axis = 1) # axis = 1 computes the mean row-wise\n","feature_dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaMgRU8noyhx"},"outputs":[],"source":["fig = plt.figure(figsize = (15, 6))\n","\n","ind = np.arange(len(cols)) \n","width = 0.15\n","\n","for i, m in enumerate(feature_dataframe.columns[1:]):\n","    plt.bar(ind + ((i - 2) * width), feature_dataframe[m], width, label = m)\n","\n","plt.title('Feature Importance')\n","plt.xticks(ind, cols, rotation = 45)\n","plt.ylabel('Score')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDBPlUemoyh0"},"outputs":[],"source":["def show_summary_report(actual, prediction):\n","\n","    if isinstance(actual, pd.Series):\n","        actual = actual.values\n","    if actual.dtype.name == 'object':\n","        actual = actual.astype(int)\n","    if prediction.dtype.name == 'object':\n","        prediction = prediction.astype(float)\n","        \n","    prediction_int = np.round(prediction)\n","\n","    accuracy_ = accuracy_score(actual, prediction_int)\n","    precision_ = precision_score(actual, prediction_int)\n","    recall_ = recall_score(actual, prediction_int)\n","    roc_auc_ = roc_auc_score(actual, prediction)\n","\n","    print('Accuracy : %.4f [TP / N] Proportion of predicted labels that match the true labels. Best: 1, Worst: 0' % accuracy_)\n","    print('Precision: %.4f [TP / (TP + FP)] Not to label a negative sample as positive.        Best: 1, Worst: 0' % precision_)\n","    print('Recall   : %.4f [TP / (TP + FN)] Find all the positive samples.                     Best: 1, Worst: 0' % recall_)\n","    print('ROC AUC  : %.4f                                                                     Best: 1, Worst: < 0.5' % roc_auc_)\n","    print('-' * 107)\n","    print('TP: True Positives, FP: False Positives, TN: True Negatives, FN: False Negatives, N: Number of samples')\n","\n","    # Confusion Matrix\n","    mat = confusion_matrix(actual, prediction_int)\n","\n","    # Precision/Recall\n","    precision, recall, _ = precision_recall_curve(actual, prediction_int)\n","    average_precision = average_precision_score(actual, prediction_int)\n","    \n","    # Compute ROC curve and ROC area\n","    fpr, tpr, _ = roc_curve(actual, prediction)\n","    roc_auc = auc(fpr, tpr)\n","\n","    # plot\n","    fig, ax = plt.subplots(1, 3, figsize = (18, 6))\n","    fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n","\n","    # Confusion Matrix\n","    sns.heatmap(mat.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Blues', ax = ax[0])\n","\n","    ax[0].set_title('Confusion Matrix')\n","    ax[0].set_xlabel('True label')\n","    ax[0].set_ylabel('Predicted label')\n","    \n","    # Precision/Recall\n","    step_kwargs = {'step': 'post'}\n","    ax[1].step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n","    ax[1].fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n","    ax[1].set_ylim([0.0, 1.0])\n","    ax[1].set_xlim([0.0, 1.0])\n","    ax[1].set_xlabel('Recall')\n","    ax[1].set_ylabel('Precision')\n","    ax[1].set_title('2-class Precision-Recall curve')\n","\n","    # ROC\n","    ax[2].plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (AUC = %0.2f)' % roc_auc)\n","    ax[2].plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')\n","    ax[2].set_xlim([0.0, 1.0])\n","    ax[2].set_ylim([0.0, 1.0])\n","    ax[2].set_xlabel('False Positive Rate')\n","    ax[2].set_ylabel('True Positive Rate')\n","    ax[2].set_title('Receiver Operating Characteristic')\n","    ax[2].legend(loc = 'lower right')\n","\n","    plt.show()\n","    \n","    return (accuracy_, precision_, recall_, roc_auc_)"]},{"cell_type":"markdown","metadata":{"id":"_-S2Bx_Moyh1"},"source":["### Summary for the First Level\n","**NOTE**: As the dataset is given from a Kaggle competition, the output values for the test set are **not** known. The summary report below are from the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJvcluROoyh2","scrolled":false},"outputs":[],"source":["# Keep the results in a dataframe\n","results = pd.DataFrame(columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'ROC_AUC'])\n","\n","models = ['Random Forest', 'Extra Trees', 'AdaBoost', 'Gradient Boost', 'Support Vector Classifier']\n","for i, m in enumerate([rf, et, ada, gb, svc]):\n","    print('*' * (len(models[i]) + 4))\n","    print('* %s *' % models[i])\n","    print('*' * (len(models[i]) + 4))\n","    predictions = m.predict_proba(x_train)[:,1]\n","    # show the report\n","    accuracy_, precision_, recall_, roc_auc_ = show_summary_report(y_train, predictions)\n","    # keep the results\n","    results.loc[i] = {'Model': models[i], \n","                      'Accuracy': accuracy_, \n","                      'Precision': precision_,\n","                      'Recall': recall_,\n","                      'ROC_AUC': roc_auc_}\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"lErFaHgSoyh4"},"source":["## Second Level Predictions from the First Level Output\n","### First level output as new features\n","One can think of building a new set of features from the first level predictions to be used as training data for the next classifier.\n","\n","The code below have the first level predictions from the earlier classifiers as new columns and trains the next classifier from those."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9N19LSsoyh4"},"outputs":[],"source":["base_predictions_train = pd.DataFrame({\n","    'RandomForest': rf_oof_train.ravel(),\n","    'ExtraTrees': et_oof_train.ravel(),\n","    'AdaBoost': ada_oof_train.ravel(),\n","    'GradientBoost': gb_oof_train.ravel()\n","})\n","print('Rows: %d, Columns: %d' % base_predictions_train.shape)\n","base_predictions_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YQWEHauoyh6"},"outputs":[],"source":["colormap = plt.cm.RdBu\n","plt.figure(figsize = (12, 12))\n","plt.title('Correlation Heatmap of the Second Level Training set', size = 15)\n","sns.heatmap(base_predictions_train.astype(float).corr(),\n","            linewidths = 0.1,\n","            vmax = 1.0,\n","            square = True,\n","            cmap = colormap,\n","            linecolor = 'white',\n","            annot = True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKkgmeAJoyh7"},"outputs":[],"source":["# Creates a new set of training and test data from the previous results results\n","xx_train = np.concatenate(\n","    (et_oof_train,\n","     rf_oof_train,\n","     ada_oof_train,\n","     gb_oof_train,\n","     svc_oof_train),\n","    axis = 1)\n","xx_test = np.concatenate(\n","    (et_oof_test,\n","     rf_oof_test,\n","     ada_oof_test,\n","     gb_oof_test,\n","     svc_oof_test),\n","    axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"BkD96Vgooyh8"},"source":["## Second Level learning model via XGBoost\n","\n","The **XGBoost** was built to optimize large-scale boosted tree algorithms.\n","\n","The XGBoost parameters used in the model:\n","\n","- **max_depth**: How deep to grow the tree. Runs the risk of overfitting if set to too high.\n","- **gamma**: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n","- **eta**: Step size shrinkage used in each boosting step to prevent overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjlX0ts2oyh9"},"outputs":[],"source":["gbm = xgb.XGBClassifier(\n","#     learning_rate = 0.02,\n","    n_estimators = 2000,\n","    max_depth = 4,\n","    min_child_weight = 2,\n","#     gamma = 1,\n","    gamma = 0.9,                        \n","    subsample = 0.8,\n","    colsample_bytree = 0.8,\n","    objective = 'binary:logistic',\n","    nthread = -1,\n","    scale_pos_weight = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVLUc3Proyh_","scrolled":false},"outputs":[],"source":["gbm.fit(xx_train, y_train)\n","predictions = gbm.predict_proba(xx_train)[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kIxbXrnoyiA"},"outputs":[],"source":["accuracy_, precision_, recall_, roc_auc_ = show_summary_report(y_train, predictions)\n","# keep the results\n","results.loc[len(results)] = {\n","    'Model': 'XGBoost', \n","    'Accuracy': accuracy_, \n","    'Precision': precision_,\n","    'Recall': recall_,\n","    'ROC_AUC': roc_auc_}\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pum4mV2YoyiB"},"outputs":[],"source":["results"]},{"cell_type":"markdown","metadata":{"id":"RERADKgNFq9T"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","> > > > > > > > > © 2022 Institute of Data\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}